# Final Report: Data Mining on Airbnb New User Bookings
## Define the problem
The problem in this project is that where would the new users book their first booking based on their personal information (gender, age, etc.) and web activity information (signup method, affiliate channel, first browser, etc.). This is a multiclass classification problem.
## Identify the client
Airbnb or other websites for vacation rentals would be the potential client. They would like to predict the places that new users mostly likely would book for first booking so that they could push notifications or send promotions about this specific place. In this case, because these places interest the new users, the new users would have a higher possibility to book in their website or to start their first booking in a shorter time.
## Describe the data set
The data set is obtained from the Airbnb new users booking Kaggle competition website. Only the train_users_2 csv file is used. It records 213,451 new users’ personal information and their web activity information as well as their country destination of first booking. It can be downloaded directly from the website: https://www.kaggle.com/c/airbnb-recruiting-new-user-bookings. There are categorical variables, numerical variables and time series variables in this data set. The output country destination contains multiple classes. A detailed information on variables is given below: 
-	id: user id
- date_account_created: the date of account creation
-	timestamp_first_active: timestamp of the first activity, note that it can be earlier than date_account_created or date_first_booking because a user can search before signing up
-	date_first_booking: date of first booking
-	gender
-	age
-	signup_method
-	signup_flow: the page a user came to signup up from
-	language: international language preference
-	affiliate_channel: what kind of paid marketing
-	affiliate_provider: where the marketing is e.g. google, craigslist, other
-	first_affiliate_tracked: whats the first marketing the user interacted with before the signing up
-	signup_app
-	first_device_type
-	first_browser
-	country_destination: target variable 
## Data wrangling
Before starting the data wrangling, I did some exploration on the data. I used bar charts, distribution plots and tables to explore the categories in each categorical variable and the relationship between some input variables and the output. Then data wrangling was done based on the results found from data exploration. 

For categorical variables, dummy variable method is used for separating categories and the first category is dropped to avoid high correlation among features. Each category is usually left alone unless the size of the category is very small and it is reasonable to combine with another or some other categories. In the first device variable, desktop(other), mobile(other) and other/unknown are combined as one group. 

There are three time series variables in the dataset. At first, I considered to add them into the features. For first active time and account create time, I calculated the gap between these two and did a value count. For the first booking time, I extracted the year and month for each entry and appended to two separate lists. For the data that does not have first booking time, I left them as nan values. However, the value count result showed that even I counted for gap days, most of the entries are still zero. The year and month for first booking should also be removed since the problem here is to predict where would a new user’s first booking be and at that time the first booking time would not be known. Hence, all the three time series variables are not included in the features. 

There is only one numerical variable: age in the input variables. Consider that about half of the entries are nan values, I did not leave this variable as numerical values but rearranged them as categories. Ages higher than 95 or lower than 18 are grouped with nan values, ages between 18 and 29 are grouped as young people, ages between 30 and 54 are grouped as middle age people and ages between 55 and 95 are grouped as old people. 
## Other potential data sets that could use
In this Kaggle competition, Airbnb also gives out the session data set which includes a lot more web activity information for new users. This data set could be concatenated with the train data set to have more features for prediction.
## Initial findings using statistics methods
Firstly, I calculated what percentage of the data set is nan values and found that nearly 60% of the new users do not have first booking dates, about 45% of the new users do not have gender values and about 42% of the new users do not have valid age values. The other variable that has nan values is first affiliate tracked but it only contains about 2% nan values. 

From the bar chart of country destination value counts, it is shown that more than half of the entries are NDF, which means the new user has not booked yet. Also, for all the new users that had their first booking, about half of them went to United States. 

For age variable, I first plotted the distribution of age for new users that went to three different country destinations: Australia, Canada and Germany. From the distribution, it is shown that these three country destinations have about the same right skewed distribution with the mode at around 30 years old. Then I also plotted the boxplots for each country destination and it also shows that the age distributions are similar. Moreover, Spain and Netherland tend to attract younger people while Germany and France tend to attract older people. 

For the time series variables, there is not much sense on keeping first active time and account create time as features. Instead, the gap between first active and account create could be an interesting feature to look at. However, more than 99% of the calculated gap month or gap days are zero. It shows that most new users created account at the same day of their first active date. First booking dates are separated as first booking years and first booking months. Value counts show that there are no extreme values or highly skewed distributions in years or months. 

For gender variable, since the category “other” has a small size, it is combined with 44% of the variable that are nan values. From the bar chart of value counts for each category in gender variable versus different country destinations, the new users that chose unknown or other as gender have much higher possibility to not have first bookings yet. Other relationships are not obvious. 

For language variable, from the bar chart of value counts for English and all other language versus different country destinations, it is shown that all country destinations were chose by a lot more new users that speak English than new users that speak other language. This could because that Airbnb is very popular in United States but might not have the same popularity in other countries that does not speak English. 

For the other categorical variables, some of them have very small category size (smaller than 100) but most of the categories have their own meanings and are not suitable to combine with other categories. 

Above all, my initial findings are that: 
1.	There are many unknown values in this dataset. 
2.	More than half of the new users do not have their first bookings and it might be appropriate to build sequential models to first predict whether the new users would make first booking and then take the new users who predicted positive to further predict where would their first booking be. 
3.	Input variables have some relationships with the output variable. 

## In-depth Analysis Using Machine Learning
### Training Algorithms:
There are three algorithms that have been used in this project: linearSVC, random forest classifier and gradient boosting classifier. LinearSVC is similar to support vector classifier with kernel set to ‘linear’. The main difference is that, in scikit-learn, linearSVC is implemented in terms of liblinear while support vector classifier is in libsvm, so bias is regularized in linearSVC but not in SVC with kernel set to linear. LinearSVC is robust in dealing with large numbers of samples. It takes much longer time for SVC with either linear or nonlinear kernels to train large scale datasets for because of the algorithm complexity. Hence, I decided to use linearSVC instead of support vector machine. Random forest classifier is chosen as this dataset has a large number of samples and not too much features. It ensembles several independent decision trees on various sub-samples and averages to improve the predictive accuracy and control overfitting. Gradient boosting classifier is also an ensemble of decision trees but it allows optimization of an arbitrary differentiable loss function. Though gradient boosting classifier would take a longer time than random forest because it builds one tree at a time, each new tree in gradient boosting helps to correct errors made by previous trees and thus can boost the accuracy score. Overfitting happens easier in gradient boosting compared with random forest but it works in large scale datasets. 

Voting classifier is used to combine the three models mentioned above. Hard voting is chosen to take the major vote from all three algorithms. 

In this project, a voting classifier combining all three models trained by 70% of the whole dataset is first used to predict whether the new users would make first bookings. Then the new users who predicted positive by the first voting classifier will be taken to the second voting classifier combining all three models trained by 70% of data entries that had first bookings to predict where would the new users make their first bookings. 
### Criteria Tool:
The criteria tool used to evaluate the model’s performance is classification report, which shows the precision, recall, f1 score and support numbers of all classes. F1 score represents the harmonic mean of precision and recall. For this typical problem, precision and recall are both important, with precision means how many percentage of people that are predicted to go to a specific country destination by the model really went to that country destination and recall means how many percentage of people that went to a specific country destination are predicted to go to that country destination by the model. Hence, all three scores are used to determine which model would be better. Since the dataset is imbalanced, I also paid more attention to the improvement of scores for classes that have small numbers of samples.
### Imbalanced Data:
This dataset is imbalanced as 58% of the new users have not booked yet, 29% of the users who had first bookings went to United States and some destinations such as Australia and Portugal have less than 1% data entries. To first deal with the imbalance that more than half of the new users have not made first bookings, I built two ensemble models and used the first ensemble model to predict whether the new users would make first bookings and then took the new users who predicted positive to the second model to predict where would the new users go. Generally speaking, it is to use a binary classification followed by a multi-class classification. To deal with the other country destinations’ imbalance, I set class_weight=’balanced’ in all the individual models that have this parameter. 
### Cross Validation:
Since the data set is large, I randomly shuffle the data and then split the data to 70% training set, 20% validation set and 10% test set. Training set is used for fitting the model, validation set is used to validate the performance of the fitted model and test set is for the final comparison between models.
### Hyperparameters Tuning:
In each model training, GridSearchCV method is used to tune hyperparameters. 5-fold cross validation is used in grid search and the scoring function uses accuracy classification score.
### Results Analysis:
In the first model that predicts whether the new users would make first bookings, I first trained linearSVC, random forest and gradient boosting models separately using 70% of the entire dataset. Then I built an ensemble of all three tuned models using voting classifier that takes the major votes. From the classification reports, it is shown that the three models have very close precision, recall and f1 scores. Furthermore, combining precision, recall and f1 scores of each model, the ensemble of three models preforms best, next is gradient boosting classifier, then random forest and linearSVC at the last. It is about the same as what I expected. 

In the second model that predicts where the new users first bookings would be, I did the same steps to first train three different models separately but using only 70% of the data entries that had first bookings and then ensemble the three models using hard voting classifier. From the classification reports, it is shown that the linearSVC and gradient boosting classifier have similar scores on total precision, recall and f1 scores. However, the gradient boosting classifier focuses a lot more on the two major classes United States and Other and many classes with small size are not even presented in the prediction. This could be due to that gradient boosting classifier do not have the class weight parameter so it performs not well in this highly imbalanced dataset. LinearSVC takes more attention on small size classes than gradient boosting, though it has slightly lower scores in the average/total scores. Random forest classifier takes the most care of all the small size classes. All the classes have non-zero scores in the classification report for random forest. However, there is a clear trade-off between caring about small size classes and improving the average/total recall and f1 scores: models that do not care for small size classes predicts a lot more major classes and thus would have a higher recall score which would also improve the f1 score. Random forest classifier got much lower recall and f1 scores in average/total. The ensemble of three models balanced the situation to keep the higher recall and f1 scores while also make sure small classes are present in the prediction. 

Lastly, the two built model ensembles are fed sequentially to the entire dataset and the classification report shows that the precision and recall have 0.62 as score and f1 scores 0.59. This is higher than the scores of the classification report for the second ensemble model. To further compare, I built one ensemble of three models that are directly trained by 70% of the entire dataset to predict where the new users would go (including ‘NDF’ which shows there is no first booking yet) and printed the classification report on the 20% test dataset. The precision, recall and f1 scores are all slightly lower than using two ensemble models sequentially. More importantly, the two model-ensembles method takes better care of classes that have very small numbers of samples than only one ensemble method. All classes are present in the prediction and have non-zero scores in classification report for the method to feed data in two model-ensembles sequentially.

